 An enterprise implementation of GlusterFS built on Red Hat Enterprise Linux and XFS
A software-defined storage appliance that can easily manage unstructured data for physical, virtual, and cloud environments
An open, scale-out storage platform available for datacenter, private cloud, and public cloud deployments


ARCHITECTURE

Module Topics

    Building Blocks

    GlusterFS Cluster Design

    1. Need for Scale-Out Storage

    2. Target Workload I/O Profiles

    3. Storage Access Methods

    4. Storage Capacity

    5. Fault-Domain Risk Tolerance

    6. Data Protection Scheme

    Target Cluster Hardware

    References


GlusterFS Cluster Design

    Determine need for scale-out storage

    Design for target workload I/O profiles

    Choose storage access methods

    Qualify storage capacity

    Determine fault-domain risk tolerance

    Select data protection scheme


1. Need for Scale-Out Storage

    Elastic provisioning across storage server cluster

    Standardized servers and networking

    Petabyte scale

    Data HA across "islands" of scale-up storage servers

    Performance and capacity scaled independently

    Incremental versus forklift upgrade


Need for Scale-Out Storage
Popular Use Cases
- Analytics
 Manage analytics with Splink
 Big data as analytics with Hadoop

Enterprise File Sharing
 - Media Streaming
 - Active Archives

Enterprise Virtualization
- Rich media and Archival

Need for Scale-Out Storage
- Server-Based Storage Capacity
- Designed for Agility
- 

Target Workload I/O Profiles

    Performance versus "cheap-and-deep"

    Throughput versus IOPS

    Small versus large block

    Sequential versus random

    Read versus write mix

    Absolute latency versus consistent targets

    Sync versus async

2. Target Workload I/O Profiles
High Performance Computing
            

General Purpose File Serving
            

Archival Use

Optimized for Performance

    Highest performance (MB/sec or IOPS)

    CapEx: Lowest $/performance-unit

    OpEx: Highest performance/BTU

    OpEx: Highest performance/watt

    Minimum server fault-domain recommendation: 1 server less than 10% of cluster


Optimized for Cost and Capacity

    CapEx: Lowest $/TB

    OpEx: Lowest BTU/TB

    OpEx: Lowest watt/TB

    OpEx: Highest TB/Rack-unit

    Minimum server-fault domain recommendation: 1 server less than 15% of cluster


4. Storage Capacity
IOPS Optimized
                
Throughput Optimized

Cost/Capacity Optimized


5. Fault-Domain Risk Tolerance

    During recovery from server failure in cluster with fewer nodes:

        Workload performance degrades

        Larger percentage of cluster’s reserve storage capacity used

    Guidelines:

Distributed or replicated volumes
    

2 nodes/cluster

Distributed and replicated volumes
    

    4 nodes/cluster

    Best for redundancy and performance

Erasure-coded volumes
    

    6 nodes/cluster

    Best capacity-overhead tradeoff; good for archival workload


6. Data Protection Scheme

    Choice greatly affects TCO

    Red Hat Gluster Storage offers two data protection schemes:

        Replication

        Erasure coding
                
Data Protection Schemes
Erasure coding
- Erasure coding is an advanced data protection mechanism that reconstructs corrupted or lost data by using information stored elsewhere in the storage system.

    Data encoded into k chunks with m parity chunks, spread across disks and servers

    Tolerates m disk failures without data loss

    Red Hat supports 8+3, 8+4, and 4+2 configurations

    Provides failure protection beyond single/double component failure

    Consumes less space than replication

    Cost-effective solution for performance requirements

    Example: 8+3 over JBOD = 73% usable:raw capacity ratio


Target Cluster Hardware
IOPS Optimized
                

Throughput Optimized
                

Cost/Capacity Optimized


Target Cluster Hardware
Architecture Overview

    Performance increases linearly with number of nodes

    Gluster performance related to file size

        Small IO not a recommended workload

    Gluster performance relies greatly on underlying hardware


Subsystem Sizing Guidelines

    For details on sizing these subsystems within clusters, see Red Hat Gluster Storage: Compatible Physical, Virtual Server and Client OS Platforms:

    https://access.redhat.com/articles/66206

        Server chassis size

        CPU

        Memory

        Disk

        Network


References

    Red Hat Gluster Storage Performance presentation from Red Hat Summit 2015:

    https://videos.cdn.redhat.com/summit2015/presentations/13767_red-hat-gluster-storage-performance.pdf

    Technical white paper: HP RA for Red Hat Storage Server on HP ProLiant SL4540 Gen8 Server:

    http://h20195.www2.hpe.com/V2/getpdf.aspx/4AA4-7192ENW.pdf?ver=1.0



Concepts

Module Topics

    What Is GlusterFS?

    What Is Red Hat Gluster Storage?

    Technology Stack

    Scaling

    Under the Hood

    Distribution and Replication

    Geo-Replication

    Gluster Native Client

    NFS Ganesha

    SMB/CIFS

    Common Solutions


What Is GLusterFS?

    POSIX-compliant distributed file system

    No metadata server

    Heterogeneous commodity hardware

    Aggregated storage and memory

    Standards-based: clients, applications, networks

    Flexible and agile scaling

    Capacity: petabytes and beyond

    Performance: thousands of clients

    Single global namespace


What Is Red Hat Gluster Storage?

    Enterprise implementation of GlusterFS

        Built on Red Hat Enterprise Linux and XFS

        Subscription model

    Software appliance for physical, virtual, cloud storage

    Bare metal installation

    Datacenter and private cloud deployments

    Amazon Web Services public cloud deployments


GlusterFS vs. Traditional Storage

    Network attached storage (NAS) limited in scalability and redundancy

    Other distributed file systems limited by metadata

    Storage area network (SAN)

        High performance and scalable

        But costly and complicated

    GlusterFS:

        Linear scaling

        Minimal overhead

        High redundancy

        Simple and inexpensive deployment


echnology Stack
Terminology

    Brick

        File system mount point

        Unit of storage used as GlusterFS building block

    Translator

        Logic between bits and global namespace

        Layered to provide GlusterFS functionality

    Volume

        Bricks combined and passed through translators

    Node

        Server running gluster daemon and sharing volumes


Foundational Components

    Private cloud (datacenter)

        Can use commodity x86_64 servers

    Public cloud

        Amazon Web Services (AWS)

        EC2 + EBS

    Red Hat Gluster Storage Hardware Compatibility List

Disk, Logical Volume Management, and File Systems

    Direct-attached storage (DAS)

    Just a bunch of disks (JBOD)

    Hardware RAID

    Logical Volume Management (LVM)

    XFS, EXT3/4, BTRFS

    Extended attributes support required

    Red Hat Gluster Storage requires:

        DAS or internal storage

        RAID 6

        XFS


Data Access Overview

    Filesystem in Userspace (FUSE)

    Gluster Native Client

    Ganesha NFS (versions 3 and 4)

    SMB/CIFS (Samba server required)


GlusterFS Components

    glusterd
    glusterfsd
    glusterfs
    mount.glusterfs
    gluster

Scaling
Scaling Up

    Adds disks and file systems to node

    Expands GlusterFS volume by adding bricks

    Downside: Individual nodes must handle added workload

Scaling Out

    Adds GlusterFS nodes to trusted pool

    Adds file systems as new bricks

    Advantage: Improved cluster performance



Under the Hood
Elastic Hash Algorithm

    Replaces central metadata

        Removes performance bottleneck

        Eliminates risk scenarios

    Location hashed intelligently on path and filename

    Unique identifiers (similar to md5sum)

    The “elastic” part

        Files assigned to virtual volumes

        Virtual volumes assigned to multiple bricks

        Volumes easily reassigned on the fly


Under the Hood
Translators


Distribution and Replication
Distributed Volume

    Files spread evenly across bricks

    File-level RAID 0

    Server/disk failure can be catastrophic


Replicated Volume

    Files copied to multiple bricks

    File-level RAID 1

    Provides redundancy at cost of performance


Distributed Replicated Volume

    Files distributed across replicated bricks

    RAID 1 plus improved read performance


Distribution and Replication
Dispersed Volume

    Based on erasure coding

    Uses less space than distributed-replicated volume

    Each brick stores some portion of data and parity or redundancy

    Sustains loss based on redundancy level


Geo-Replication

    Asynchronous across LAN, WAN, and Internet

    Master-slave model, cascading possible

    Continuous and incremental


    Synchronize time on all master nodes


Cascading relationships are possible between multiple sites for disaster recovery or data synchronization
Geo-replication provides asynchronous replication between nodes across LAN, WAN, and Internet connections


Replicated Volumes vs Geo-Replication

Mirrors data across cluster nodes
    

Mirrors data across geographically distributed clusters

Provides high availability
    

Ensures data backup for disaster recovery

Synchronous replication (every file operation sent across all bricks)
    

Asynchronous replication (periodically checks for changes in files and syncs when detected)


Gluster Native Client

    FUSE kernel module allows file systems to be built and operated in user space

    Specify mount to any GlusterFS node

    Native Client fetches volfile from mount server, then communicates directly with all nodes to access data

    Recommended for high concurrency and high write performance


NFS-Ganesha

    Supports standard NFS v3 and v4 clients

    Supports standard auto-mounter

    Uses floating IP address to handle failover gracefully

    Includes Network Lock Manager (NLM) to synchronize locks across clients

    Provides user space file system with better performance than kernel NFS


SMB/CIFS

    GlusterFS volume is first redundantly mounted with Native Client on node

    Native mount point is then shared via Samba

    Must be set up on each node to connect to via CIFS


