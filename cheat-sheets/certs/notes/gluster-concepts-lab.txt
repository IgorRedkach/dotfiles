Gluster Concepts Lab

In this set of exercises, you explore Red Hat Gluster Storage features and concepts with local and remote storage clusters, mount volumes using multiple protocols, work with volume snapshots, and use the console to configure geo-replication.
1. Access Lab Environment

Perform this lab as root on the rhgs1.localdomain storage node. Remember that you must first connect to client1 before you can use SSH to access the storage nodes.

    Use the SSH client of your choice to log in to client1-GUID.rhpds.opentlc.com (external name):

    # ssh [OPENTLC-SSO-LOGIN]@client1-GUID.rhpds.opentlc.com

        Replace GUID with the GUID provided in your provisioning email.

        Use your OPENTLC SSO login name and your private key to authenticate.

        If needed, you can use # sudo su - at any time to become root on the client1 system.

    Log in remotely to rhgs1 host from client1:

    [root@client1 ~]# ssh root@rhgs1

        User: root

        Password: Redhat12

2. Manage Gluster Daemon

In this exercise, you manage the Red Hat Gluster Storage daemon. You start, stop, and check the status of the glusterd service.
2.1. Stop and Restart the Gluster Storage Daemon

When Red Hat Gluster Storage is installed, the glusterd service starts automatically. Use the following instructions to manually check glusterd status, stop the service, and verify its status again.

    Check the status of the glusterd service:

    [root@rhgs1 ~]# systemctl status glusterd
    [green]#●# glusterd.service - GlusterFS, a clustered file-system server
       Loaded: loaded (/usr/lib/systemd/system/glusterd.service; enabled; vendor preset: disabled)
       Active: [green]#active# (running) since Sun 2016-08-28 18:27:02 EDT; 31min ago
     Main PID: 6387 (glusterd)
       CGroup: /system.slice/glusterd.service
               ├─6387 /usr/sbin/glusterd -p /var/run/glusterd.pid
               ├─7825 /usr/sbin/glusterfsd -s rhgs1 --volfile-id localvol.rhgs1.gluster-brick1 -p /var/lib/glusterd...
               ├─7858 /usr/sbin/glusterfs -s localhost --volfile-id gluster/nfs -p /var/lib/glusterd/nfs/run/nfs.pi...
               ├─7865 /usr/sbin/glusterfs -s localhost --volfile-id gluster/glustershd -p /var/lib/glusterd/gluster...
               └─7889 /sbin/rpc.statd

    Manually stop glusterd:

    [root@rhgs1 ~]# systemctl stop glusterd

    Check the status of glusterd again:

    [root@rhgs1 ~]# systemctl status glusterd
    ● glusterd.service - GlusterFS, a clustered file-system server
       Loaded: loaded (/usr/lib/systemd/system/glusterd.service; enabled; vendor preset: disabled)
       Active: inactive (dead) since Sun 2016-08-28 19:02:52 EDT; 17s ago
     Main PID: 6387 (code=exited, status=0/SUCCESS)
       CGroup: /system.slice/glusterd.service
               ├─7825 /usr/sbin/glusterfsd -s rhgs1 --volfile-id localvol.rhgs1.gluster-brick1 -p /var/lib/glusterd...
               ├─7858 /usr/sbin/glusterfs -s localhost --volfile-id gluster/nfs -p /var/lib/glusterd/nfs/run/nfs.pi...
               ├─7865 /usr/sbin/glusterfs -s localhost --volfile-id gluster/glustershd -p /var/lib/glusterd/gluster...
               └─7889 /sbin/rpc.statd

2.2. Start Red Hat Gluster Storage Daemon

In this section, you restart the glusterd service and verify that it is running.

    Manually start glusterd:

    [root@rhgs1 ~]# systemctl start glusterd

    Check glusterd status to make sure it is running:

    [root@rhgs1 ~]# systemctl status glusterd
    Redirecting to /bin/systemctl status  glusterd.service
    [green]#●# glusterd.service - GlusterFS, a clustered file-system server
       Loaded: loaded (/usr/lib/systemd/system/glusterd.service; enabled; vendor preset: disabled)
       Active: [green]#active (running)# since Sun 2016-08-28 19:04:49 EDT; 35s ago
      Process: 27068 ExecStart=/usr/sbin/glusterd -p /var/run/glusterd.pid (code=exited, status=0/SUCCESS)
     Main PID: 27073 (glusterd)
       CGroup: /system.slice/glusterd.service
               ├─ 7825 /usr/sbin/glusterfsd -s rhgs1 --volfile-id localvol.rhgs1.gluster-brick1 -p /var/lib/gluster...
               ├─27073 /usr/sbin/glusterd -p /var/run/glusterd.pid
               ├─27536 /usr/sbin/glusterfs -s localhost --volfile-id gluster/nfs -p /var/lib/glusterd/nfs/run/nfs.p...
               ├─27552 /usr/sbin/glusterfs -s localhost --volfile-id gluster/glustershd -p /var/lib/glusterd/gluste...
               └─27570 /sbin/rpc.statd

3. Examine Primary (Local) Storage Cluster

In this exercise, you examine a two-way replicated volume called localvol and verify its status from the rhgs1 storage node. This volume was created using the four primary hosts rhgs1, rhgs2, rhgs3, and rhgs4.
3.1. View Configuration File

The configuration file requests the creation of Red Hat Gluster Storage bricks using the /dev/vdb drive on each node, mounting them under /gluster/brick1, formatted as an XFS file system.

    As root on rhgs1, view the configuration file used by gdeploy to set up the primary Red Hat Gluster Storage cluster and localvol volume:

    [root@rhgs1 ~]# cd gdeploy_conf
    [root@rhgs1 gdeploy_conf]# cat local.conf
    [hosts]
    rhgs1
    rhgs2
    rhgs3
    rhgs4

    [devices]
    /dev/vdb

    [tune-profile]
    none

    [peer]
    manage=probe
    # manage=detach

    [volume]
    action=create
    volname=localvol
    transport=tcp,rdma
    replica=yes
    replica_count=2
    # arbiter_count=2
    force=yes

    Review the [hosts] and [devices] sections carefully.

    Note that in the [volume] section, replica=yes and replica_count=2 define localvol as a two-way replicated volume.

3.2. Verify the Volume

    Display the block storage devices on rhgs1 to confirm that the /gluster/brick1 brick is mounted on /dev/vdb:

    [root@rhgs1 gdeploy_conf]# lsblk

    lsblk rhsg1

    Verify the cluster’s status:

    # gluster peer status

    Confirm that the Red Hat Gluster Storage volume is started and has a Started status:

    # gluster volume info

    Review your output and verify that it looks similar to this:
    gluster volume localvol info

4. Examine Secondary (Remote) Storage Cluster

In this exercise, you examine a second two-way replicated volume called remotevol from the remote-rhgs1 storage node. This volume was created with bricks on the two remote hosts remote-rhgs1 and remote-rhgs2.
4.1. View Configuration File

The configuration file requests the creation of Red Hat Gluster Storage bricks using /dev/vdb and /dev/vdc drives on each storage node, mounting them at /gluster/brick1 and /gluster/brick2, formatted as XFS file systems.

    As root on rhgs1, view the configuration file used by gdeploy to set up the secondary Red Hat Gluster Storage cluster and remotevol volume:

    [root@rhgs1 gdeploy_conf]# cat remote.conf
    [hosts]
    remote-rhgs1
    remote-rhgs2

    [devices]
    /dev/vdb
    /dev/vdc

    [tune-profile]
    none

    [peer]
    manage=probe
    # manage=detach

    [volume]
    action=create
    volname=remotevol
    transport=tcp,rdma
    replica=yes
    replica_count=2
    # arbiter_count=2
    force=yes

    Review the [hosts] and [devices] sections.

    Note that in the [volume] section, replica=yes and replica_count=2 define remotevol as a two-way replicated volume.

4.2. Verify the Volume

    Connect to remote-rhgs1 using SSH to check the status:

    root@rhgs1 gdeploy_conf]# ssh root@remote-rhgs1

    Display the block storage devices on remote-rhsg1 to confirm that both bricks were created and mounted as /gluster/brick[1,2]:

    [root@remote-rhgs1 ~]# lsblk

    lsblk remote rhsg1

    Verify the status of the cluster:

    # gluster peer status

    Confirm that the gluster volume is created and that its status is Started:

    # gluster volume info

    Review your output and verify that it looks similar to this:
    gluster volume remotevol info

5. Access Volumes From Client

For this exercise, you install the Red Hat Gluster Storage Native Client on client1 and use it to access the localvol and remotevol volumes.
5.1. Install Red Hat Gluster Storage Native Client

    Open an SSH session to client1-GUID.rhpds.opentlc.com.

        Replace GUID with the GUID provided in your provisioning email.

    Start a root shell:

    [user@client1-GUID ~]$ sudo su -

    Install the Native Client packages:

    [root@client1-GUID ~]# yum -y install /root/gluster-client/*.rpm

    install gluster native client

    Mount the localvol and remotevol volumes using the Native Client protocol:

    # mkdir /mnt/local
    # mkdir /mnt/remote
    # mount -t glusterfs -oacl rhgs1:/localvol /mnt/local
    # mount -t glusterfs -oacl remote-rhgs1:/remotevol /mnt/remote

        The volumes are mounted with Access Control List (ACL) support to allow read/write access to Windows users.

    Verify that the volumes are mounted:

    # df -h

    mount gluster volumes

    Make a test directory under the localvol mount point and create 100 files:

    # cd /mnt/local
    # mkdir test1
    # cd test1
    # for i in `seq 1 100`
    do
    echo hello$i > file$i
    done
    # ls

    localvol 100 test files

        The view from client1 shows that all 100 files are available from the volume mount point.

5.2. Examine File Layout on Back-End Storage Bricks

In the previous exercise, you confirmed that client1 can see all 100 files.

Now examine how these 100 files are laid out on the back-end storage bricks. The view from the rhgs1 storage node under the /gluster/brick1/test1 brick directory shows that there are 54 files on this brick.

    Connect to rhgs1 to examine the brick1 directory:
    rhgs1 brick1 54 files

    Verify that you see the same 54 files on rhgs2:
    rhgs2 brick1 54 files

        Remember that rhgs1 and rhgs2 make up a mirrored pair and contain the same files.

    Verify that the brick directories on the rhgs3/rhgs4 pair have the remaining 46 files:
    rhgs3 brick1 46 files
    rhgs4 brick1 46 files

        Note that because the volume is in two-way replicated mode, the files on rhgs1 and rhgs2 are identical, as are the files on rhgs3 and rhgs4.

6. Explore Snapshots

The snapshot feature enables you to create point-in-time copies of Red Hat Gluster Storage volumes, which you can use to protect data. You can directly access read-only snapshot copies to recover from accidental deletion, corruption, or modification of the data.
snap lab diagram

When a volume snapshot is taken, another thinly provisioned brick is created. The snapshot brick is an identical image of the original brick and belongs to the same volume group. These newly created bricks combine to form a snapshot volume.

The syntax for creating a volume snapshot is:

# snapshot create SNAPNAME VOLNAME [no-timestamp] [description DESCRIPTION] [force]

6.1. Create Snapshot

In this section, you create a snapshot called snap1 and use the no-timestamp option to avoid having a timestamp appended to the name of the snapshot.

    On rhgs1, create a snapshot of localvol called snap1:

    # gluster snapshot create snap1 localvol no-timestamp

    List all of the snapshots for localvol:

    # gluster snapshot list localvol

    View detailed snapshot information for snap1:

    # gluster snapshot info snap1

    create snap1

    Activate the snapshot so you can use it:

    # gluster snapshot activate snap1

        Note that the default snapshot status is Stopped.

6.2. Recover File From Deleted Directory

In this section, you recover a file from a deleted directory using the snapshot of the original Red Hat Gluster Storage volume.

    On client1, list the contents of /mnt/local and /mnt/local/test1:

    # ls /mnt/local/
    # ls /mnt/local/test1

    Remove the /mnt/local/test1 directory and confirm that it does not exist:

    # rm -rf /mnt/local/test1
    # ls /mnt/local/test1

    Mount the snap1 snapshot using the Red Hat Gluster Native protocol under /mnt/snap:

    # mkdir /mnt/snap
    # mount -t glusterfs rhgs1:/snaps/snap1/localvol /mnt/snap/

    Check whether you can access the deleted test1 directory and its contents under /mnt/snap/test1:

    # df -h
    # ls /mnt/snap/test1/

    Confirm that your output from the preceding four steps looks similar to this:
    mount snap1

    On client1, create a file in the localvol volume:

    [root@client1-GUID ~]# ls /mnt/local/
    [root@client1-GUID ~]# touch /mnt/local/hello.world
    [root@client1-GUID ~]# ls /mnt/local/
    hello.world

    On client1, unmount the localvol volume:

    [root@client1-GUID ~]# umount /mnt/local

    On rhgs1, stop the localvol volume:

    [root@rhgs1 ~]# gluster volume stop localvol
    Stopping volume will make its data inaccessible. Do you want to continue? (y/n) y
    volume stop: localvol: success

    On rhgs1, restore the snap1 snapshot and restart the localvol volume:

    [root@rhgs1 ~]# gluster snapshot restore snap1
    Restore operation will replace the original volume with the snapshotted volume. Do you still want to continue? (y/n) y
    Snapshot restore: snap1: Snap restored successfully
    [root@rhgs1 ~]# gluster volume start localvol
    volume start: localvol: success

    On client1, mount the localvol volume again and list its contents:

    [root@client1-GUID ~]# mount -t glusterfs rhgs1:/localvol /mnt/local
    [root@client1-GUID ~]# ls /mnt/local/
    [root@client1-GUID ~]# ls /mnt/local/test1
    [root@client1-GUID ~]# ls /mnt/local/hello.world

    Confirm that the localvol volume with test1 directory and 100 test files are there, but that the hello.world file is not.

        Remember, the volume snapshot was created before the hello.world file was added to the volume.

7. Using Network Services

In this exercise, you mount the same Red Hat Gluster Storage volumes on client1 using the NFSv3 and SMB protocols.
7.1. Mount Red Hat Gluster Storage Volume Using NFS

In this section, you remain on client1 and mount localvol using the NFSv3 protocol. First you create a directory called /mnt/nfs to mount the volume. After mounting the volume with NFS, you confirm that both the NFS client and the Red Hat Gluster Storage Native Client are able to view the same data set.

    On client1, mount localvol using NFS:

    # mkdir /mnt/nfs
    # mount -t nfs -o vers=3,mountproto=tcp rhgs1:/localvol /mnt/nfs

        You can use showmount -e rhgs1, if needed, to confirm which NFS export path to use.

        You now have two mounts of localvol; one with Native Client and one with NFS.

    Verify the mounts:

    # df -h

    Change directories to /mnt/nfs/test1 and list the contents:

    # cd /mnt/nfs/test1
    # ls

    Confirm that your output looks similar to this:
    nfs mount

7.2. Mount Red Hat Gluster Storage Volume Using SMB

In this section, you mount the same localvol volume as an SMB (CIFS) share.

    On client1, list Samba volumes available from rhgs1:

    # smbclient -L rhgs1 -U samba-user

    Confirm that your output looks similar to this:
    smbclient

        The SMB configuration file adds gluster- to the beginning of the Red Hat Gluster Storage volume name.

    On client1, create a mount point, mount the volume using CIFS, and verify your results:

    # mkdir /mnt/cifs
    # mount -t cifs -ouser=samba-user //rhgs1/gluster-localvol /mnt/cifs
    Password for samba-user@//rhgs1/gluster-localvol:  ********
    # df -h
    # cd /mnt/cifs/test1
    # ls

    cifs mount

        Note in the df -h output that the same single storage volume, localvol, is now available on client1 under three separate mount points using three different protocols:

            Native Client mount (glusterfs direct mount)

            NFSv3

            SMB (CIFS)

7.3. Bonus: Mount SMB Share on Windows Host

This exercise is optional. In the following steps, you attach the SMB (CIFS) share as a Windows network drive using the Windows client system.

    Connect to rhswincli-GUID.rhpds.opentlc.com, the Windows Client system.

    Use a Remote Desktop Protocol (RDP) client to log in to rhswincli-GUID.rhpds.opentlc.com using the Windows login credentials.
        If you are not using a Windows computer, be sure to use a client application that supports Microsoft’s RDP protocol to make the RDP connection.

    Enter the external IP address for the Windows host found in your provisioning email.

        Use Administrator for the user ID and RedHat12 for the password.

            Note that the password is case-sensitive—both R and H must be uppercase.

    In Windows Explorer, right-click Computer → Map Network Drive:
    smb map network drive 1

    In the Map Network Drive dialog, select an available drive letter from the Drive list.

    In the Folder field, enter the IP address of the rhgs1 storage node and gluster-localvol, separated by a \:

    \\10.100.1.11\gluster-localvol

        Because the SMB configuration file adds gluster- to the beginning of the Red Hat Gluster Storage volume name, you must include this prefix.
        The screenshot below shows the default volume name of gluster-glustervol. Be sure to use gluster-localvol instead!
    smb map network drive 2

    In the Enter Network Password dialog, enter samba-user for the username and r3dh4t1! for the password, and click OK:
    smb login

        A Windows Explorer window appears and displays the contents of the mounted SMB share:
    smb mounted

        This is the fourth connection to the same Red Hat Gluster Storage volume, localvol. To review, three connections are from a Linux client—using Native Client, NFS, and SMB—and the fourth is an SMB connection from a Windows client.

7.4. Bonus: Mount a remotevol Volume

This exercise is optional.

    For extra practice in mounting a volume with multiple protocols, perform one or more of the previous three exercises using the remotevol volume.

8. Using the Console

In this exercise, you enable management of the storage clusters from the Red Hat Gluster Storage Console.
8.1. Log In to Administration Portal

    Access your Administration Portal at https://console-GUID.rhpds.opentlc.com using a web browser.

    If prompted by your browser, accept the SSL certificate.

    Select Administration Portal:
    admin portal

    Enter admin in the User Name field and r3dh4t1! in the Password field.

    Select internal from the Domain list.

    Click Login.

8.2. Add Local Cluster to Console

    Select Clusters from the Details pane and click New to open the New Cluster dialog.

    Check the Import existing gluster configuration box to import the localvol local cluster.

    In the Address field, enter rhgs1 (the hostname of one of the storage nodes) and click OK:
    new cluster local

        The Add Hosts window opens and all of the storage nodes that are part of the cluster are displayed.

    Check the Use a common password box.

    Enter Redhat12 in the Root Password field and click Apply.
    add hosts local

    Click OK to add the cluster.

8.3. Add Remote Cluster to Console

    Repeat the previous procedure to add the remotevol remote cluster, entering remote-rhgs1 for Address:
    new cluster remote

        The Add Hosts window opens and all of the storage nodes that are part of the cluster are displayed.

    Check the Use a common password box.

    Enter Redhat12 in the Root Password field and click Apply.
    add hosts remote

    Click OK to add the cluster.

    Confirm the online status of both the local and remote clusters:
    dashboard

9. Using Geo-Replication

In this exercise, you set up geo-replication between the localvol and remotevol Red Hat Gluster Storage volumes using the console you configured previously. Geo-replication provides a continuous, asynchronous, and incremental replication service between Red Hat Gluster Storage volumes over a LAN or WAN.
9.1. Configure Geo-Replication

Geo-replication uses a master-slave model. In the lab environment, the localvol volume is the master and remotevol is the slave.

    On client1, check the state of localvol and remotevol mount points before you enable geo-replication:

        Confirm that the 100 test files are still present in the test1 directory on localvol:

        [root@client1 ~]# cd /mnt
        [root@client1 mnt]# ls local/test1

        Confirm that remotevol is empty:

        [root@client1 mnt]# ls remote/test1

        localvol remotevol before

    Log in to the Red Hat Gluster Storage Console Administration Portal at https://console-GUID.rhpds.opentlc.com (if you are not already logged in):
    admin portal

        Use admin for the user name and r3dh4t1! for the password.

    Select Volumes, select localvol, and click Geo-replication.

    Click New.

    In the New Geo-Replication Session dialog, do the following:

        For Destination cluster, select remote.

        For Destination volume, select remotevol.

        For Destination host, select remote-rhgs1.

        Make sure the Auto-start geo-replication session after creation box is checked.

            This causes the session to start immediately after creation.
            new geo replication

    Click OK.

        Note that Geo-replication has a status of INITIALIZING:
        initializing

        After a few minutes, Geo-replication transitions to ACTIVE status:
    active

9.2. Test Geo-Replication

In this section, you check to make sure geo-replication is working.

    Once geo-replication becomes ACTIVE in the console, list the contents of localvol and remotevol again:

    [root@client1 mnt]# ls local/test1
    [root@client1 mnt]# ls remote/test1

        Note that the the geo-replication process copied the test directory and its files to the remotevol mount point:
    localvol remotevol after

    Make a new test directory under the localvol mount point and create 100 new files:

    [root@client1 ~]# cd /mnt/local
    [root@client1 local]# mkdir test2
    [root@client1 local]# cd test2
    [root@client1 test2]# touch georep{1..100}
    [root@client1 test2]# ls

    After a few seconds, list the files on remotevol:

    [root@client1 test2]# ls /mnt/remote/test2

        Note that the geo-replication process also copies the new test directory and its files to the remotevol mount point:
    local to remote
