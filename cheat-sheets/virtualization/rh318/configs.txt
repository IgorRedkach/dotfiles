- Check status of ovirt-engine
# service ovirt-engine status

- verify an ISO nfs share is available
# showmount -e localhost

- While Authorization is handled by RHEV, Authentication is handled by an external directory service
- Available directory services are:
    : Microsoft Active Directory
    : Red Hat identity services (idM)
    : Red Hat Directory Server 9
    : openLDAP
- Not necessary to install RHEVM and IdM on same system: conflict around mod_ssl package

- Bind RHEV-M to external directory service for additional user definitions

# engine-manage-domains add --domain=DOMAIN --user=USER --provider=PROVIDER
# engine-manage-domains add --domain=DOMAIN --user=USER --provider=PROVIDER
 --password-file=PASSWORD_FILE

- LDAP provider is case insensitive, van be
    : ad > Microsoft Active Directory
    : ipa > freeIPA
    : oldap > OpenLDAP
    : rhds > Red Hat Directory Server

- Then
# service ovirt-engine restart
# engine-manage-domains list

-Users will be identified by their User Principal Name (UPN) of the fprm user@domain
- Add rhevadmin user to domain 

- Reset admin@local password
# engine-config -s AdminPassword=interactive
# service ovirt-engine restart

- System Time settings
# service ntpd stop
# ntpdate ntp.example.com
# grep server /etc/ntp.conf
# service ntpd start
# hwclock --systohc

- Retrieve all existing values of RHEV installation
# engine-config -a 

- retriev value of particular key
# engine-config -g "AuditLogCleanupTime"

- Change values
# engine-config -l | grep -i timeout
# engine-config -s UserSessionTimeout=1
# engine-config -g  UserSessionTimeout
# service ovirt-engine restart

- Timeout time is in minutes

Using log collector
- The command output will be saved in the /tmp/logcollector/ directory

# engine-log-collector list
# engine-log-collector collect

- Config: /etc/rhevm/logcollector.conf
- oVirt engine log files: /var/log/ovirt-engine

Remove RHEV Manager

# engine-cleanup > Shut down RHEV-M and remove its configurations

- Remove all relevant RPMs
# yum remove rhevm* vdsm-bootstrap *jboss* postgresql-server

- Remove old ovirt-engine directories and any PostgreSQL databases, this incluse:
    : /etc/ovirt-engine
    : /usr/share/ovirt-engine*
    : /var/lib/pgsql/
- Finally, clean up any NFS shares created by the configuration tool
- Remove all relevant entries in /etc/exports and reload the nfs service to reload its configurations

# vim /etc/exports
# service nfs reload

Upgrade RHEV Hypervisor ( /usr/share/rhev-hypervisor7)
- On Rhev manager
# yum -y install rhev-hypervisor7
- Shut down all virtual machines 
- Place hypervisor in maintenance under hosts tab
- Choose upgrade on right click
- Select latest RHEV-H iso from dropdown menu and click OK
- Hypervisor will reboot once upgrade is complete

> The default time RHEV will wait bedfore checking on a RHEV-H after rebot is 300 seconds (5 mins
# tail -f /var/log/ovirt-engine/engine.log > waiting 300 seconds
- To change this value
# engine-config -s ServerRebootTimeout=90 > Change to 90 seconds
# service ovirt-engine restart


Creating RHEV Data Centers and Clusters
- Storage domain belongs directly to a data center
- Physical Host belongs to one cluster which belongs to one datacenter
- RHEV-M system may manage multiple datacebters
- A datacenter may contain multiple clusters, and a cluster may contain multiple hosts

Datacenter:
    : collection of a number of clusters of virtual machines, storage, and   
    networks
    : Highest level container for all physical and logical resources within
    a managed virtual environment

Storage:
        : Each storage pool may contain several storage domains
        : Each storage domain may contain virtual machine disks, ISO images
          or can be used for import and export of virtual machine images
        : iSCSI, NFS or Fibre Channel standalone image repository types

Cluster:
        : Set of physical hosts that are treated as a resource pool for a 
        a set of virtual machines
        : Hosts in a cluster share same network infrastructure and same 
        storage
        : They are a  migration domain within which virtual machines can be 
        moved from one host to other host

Host:
        : Physical server running either RHEV-H or RHEL and hosts one or 
          more VMs
        : RHEV-H nodes register when they are installed but cannot be used 
          until they're approved through the RHEV-M console.

Pools:
        - Group of identical virtual systems that are available on demand 
        by each of the users ( not concurrently)

Template:
         - A model virtual machine with its own set of configurations and
         settings.
         - A virtual machine that is based on a particular template acquires
         the configurations and settings of the template

Logical network:    
                - A datacenter can set up multiple logical networks to 
                segregate different types of traffic between nodes to 
                different networks
                - Segregation is done  based on the physical topology of the
                network and various functional requirements.

Snapshot:
            - View of virtual machine's os amd all its apps at a given point
             in time.
            - Can be used to save the settings of a virtual machine before 
            an upgrade or before before new apps are installed.

Events and monitors:
            - Alerts,warnings,and notices about activities within the system

Reports:    
        - RHEV-M provides warehouse that collects monitoring data for hosts,
        virtual machines, and storage that allows for the creation of 
        reports using any query tool that supports SQL

System Schedular
                 : Manages allocation of physical resources within the dc and
                  is responsible for placement of virtual machines (VMs) on
                  host systems.

Storage Container:
                    - There are three basic types of storage domains:
            : Data domains - store guest images(virtual hard disk images) 
            and snapshots of image states.
            - cannot be shared btw dcs
            - May be NFS,iSCSI or Fibre Channel-based. One of these tech may
             be used at a time for master domains in particular dc

            : ISO domains - Store ISO images (logical CD-ROMS and DVD-ROMs)
             and VFDs ( virtual floppy drive images) that are used for 
             installation or rescue of vms. can be shared across dcs.


            : Export domains - used to copy or move images btw dcs and
              RHEV-M installations. can be used to backup vms
               - Can be moved btw dcs but can only be active at one dc at
               a time

- One of the hosts in the dc is automatically elected the Storage Pool
Manager (SPM) and responsible for managing changes to the storage pool comm from RHEV-M.
- If current SPM goes down, RHEV-M will relocate the SPM to another host.

Virtual Machines:

    vCPU: max of 160 physical CPUs are supported by hypervisor. Max of 160
          vCPUs can be prested to virtual guest.
    
    vRAM: hypervisor support upto 2TB of physical RAM. 64-bit guest max
          of 3TB of virtual RAM, 32-bit guest - 4GB
    
    vNICs: upto 8 per guest
    
    PCI slots: upto 32 per guest

Fencing Policy:
                - Allows admins to set up fencing for the cluster. Fencing 
                is the process of isolating a node of a computer cluster 
                when a host appears to be malfunctioning.

Steps:
1. Create datacenter
2. Create a new cluster
3. Add Hypervisor Hosts
4. Configure storage
5. Attach storage

Uploading an image/iso to storage domain/datastore

# engine-iso-uploader -i isoX upload rhel-xxx.iso

Logical networks
- Default in all datacenters is the mgt network called rhevm.
- It is meant especially for mgt communication between the RHEV-M and Hosts
- Logical network is a datacenter-level resource; creating one in a data center makes it available to the clusters in a datacenter
- A logical network may belong to multiple clusters and this is one way for for virtual machines and nodes in multiple clusters to comm with each other.
- Each cluster in turn, may have a different set of logical networks available on it.
- All logical networks in use must be defined by the datacenter

Common practice: Create 3 logical networks with corresponding physical segregation:

1. A "public network" connecting the gateway router, the RHEV-M system, and the RHEV-M nodes
2. A "management network" connecting the RHEV-M system to the RHEV-H nodes, but is not routed. For security reasons, one could restrict ssh logins to management network
3. A "storage network" connecting the RHEV-H nodes to the NFS or iSCSI storage domains and is also connected to the RHEV-M node. FC storage uses a SAN and not IP network for comm hence separate network would not be possible.

ALL NODES in a cluster must have same network config.

Configure Mac address range pool
# engine-config -l | grep -i mac > Find RHEV settings for MAC address ranges
# engine-config -g MacPoolRanges > Verify current settings
# engine-config -s MacPoolRanges=start-end
# service ovirt-engine restart
=: DHCP server can be configured to provide ip addresses based on MAC addresses


Installing new VM
> Host requires spice client to connect to vm
# yum -y install spice-xpi

> New
> Right click > Run once
> Template if available
> Attach cdrom iso


